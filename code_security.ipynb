{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_security_duygu.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTJF4sHVGhkc","executionInfo":{"status":"ok","timestamp":1631087523183,"user_tz":-180,"elapsed":5780,"user":{"displayName":"berkehan öztürk","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08932554033601305485"}},"outputId":"55d94c1e-b4b5-487b-b9b5-0dfe8e55e03d"},"source":["!pip install tokenizers\n","!pip install --upgrade gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_Aqnt99SHaw","executionInfo":{"status":"ok","timestamp":1631087525925,"user_tz":-180,"elapsed":2746,"user":{"displayName":"berkehan öztürk","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08932554033601305485"}},"outputId":"4ac3c3fd-a009-4014-9b07-8fdfdbcf2aed"},"source":["!pip install numpy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.2)\n"]}]},{"cell_type":"code","metadata":{"id":"u_H6yPzfUIGL"},"source":["import math\n","import sys\n","import time\n","import datetime\n","import copy\n","\n","from gensim.models import KeyedVectors, Word2Vec\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, precision_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import torch.optim as optim\n","from datetime import datetime as dt\n","from openpyxl import load_workbook\n","from openpyxl.styles import Font\n","\n","from tokenizers import Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jkx_OKKpE_Fn"},"source":["excel_name = 'results'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sP-Q3-sAUz6m","executionInfo":{"status":"ok","timestamp":1631087525927,"user_tz":-180,"elapsed":17,"user":{"displayName":"berkehan öztürk","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08932554033601305485"}},"outputId":"05af91b9-5008-412a-b9eb-060f99c6a37c"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"q4M7D0ifULHB"},"source":["PAD_IDX = 0\n","UNK_IDX = 1\n","\n","global word2index\n","global embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DEZWVYKUPjh"},"source":["def pad_sents(sents, pad_idx):\n","    \"\"\"\n","    Pad the sentences with respect to max length sentence.\n","    \"\"\"\n","    max_len = max([len(sent) for sent in sents])\n","    padded_sents = []\n","    for sent in sents:\n","        if len(sent) < max_len:\n","            sent = sent + [pad_idx] * (max_len - len(sent))\n","\n","        padded_sents.append(sent)\n","\n","    return padded_sents\n","\n","\n","def word2indices(sents, word2index):\n","    \"\"\"\n","    Convert list of tokenized sentences into the list of indices. \n","    \"\"\"\n","    return [[word2index[token] if token in word2index else UNK_IDX for token in sent] for sent in sents]\n","\n","\n","#     return [[get_token_idx(word2index, token, UNK_IDX) for token in sent] for sent in sents]\n","\n","def indices2word(sents, index2word):\n","    \"\"\"\n","    Convert list of token id's into the list of sentences.\n","    \"\"\"\n","    return [[index2word[token] for token in sent] for sent in sents]\n","\n","\n","def to_tensor(sents, pad_idx=0, device=torch.device(\"cuda\")):\n","    \"\"\"\n","    Convert list of sents into list of indices.\n","    \"\"\"\n","    sent_indices = word2indices(sents, word2index)\n","    padded_sents = pad_sents(sent_indices, pad_idx)\n","    sent_tensor = torch.tensor(padded_sents, dtype=torch.long, device=device)\n","    return sent_tensor  # (batch_size, max_seq_len)\n","\n","\n","def generate_sent_masks(sents, lengths, device):\n","    \"\"\"\n","    Generate the padding masking for given sents from lenghts. \n","    Assumes lengths are sorted by descending order.\n","    \"\"\"\n","    max_len = lengths[0]\n","    bs = sents.shape[0]\n","    mask = torch.arange(max_len).expand(bs, max_len) < lengths.unsqueeze(1)\n","    return ~mask.bool().to(device)\n","\n","\n","def batch_iter(data, bs, shuffle=False):\n","    \"\"\"\n","    Yields batches of sentences reverse sorted by length (longest to smallest)\n","    \"\"\"\n","\n","    batch_num = math.ceil(len(data) / bs)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * bs: (i + 1) * bs]\n","        samples = [data[idx] for idx in indices]\n","        samples = sorted(samples, key=lambda sample: len(sample[0]), reverse=True)\n","        sents = [sample[0] for sample in samples]\n","        labels = [sample[1] for sample in samples]\n","        yield sents, labels\n","\n","\n","def load_wv(filename, fasttext=True, limit=None):\n","    if fasttext:\n","        binary = True if filename.endswith('.bin') else False\n","        print(filename, \"binary: \", binary)\n","        w2v = KeyedVectors.load_word2vec_format(filename,\n","                                                binary=binary,\n","                                                limit=limit,\n","                                                unicode_errors='ignore')\n","    else:\n","        w2v = Word2Vec.load(filename)\n","\n","    return w2v\n","\n","\n","def masked_pooling(hidden_states, masks):\n","    \"\"\"\n","    Generate the pooled output.\n","    hidden_states: Hidden state output of the RNN-like model.\n","    masks: Sent masks for the padding. \n","    \"\"\"\n","    avg_pool = hidden_states.masked_fill(masks[:, :, None], 0).mean(dim=1)\n","    avg_pool *= hidden_states.size(1) / (hidden_states.size(1) - masks.type(avg_pool.dtype).sum(dim=1))[:, None]\n","    max_pool = hidden_states.masked_fill(masks[:, :, None], -float('inf')).max(dim=1)[0]\n","\n","    return max_pool, avg_pool\n","\n","\n","def bn_drop_linear(n_in, n_out, bn=True, p=0.0, activation=None):\n","    \"\"\"\n","    Sequence of BatchNorm if bn=True, Dropout if p>0 and Linear(n_in, n_out) layers followed by activation.\n","    \"\"\"\n","\n","    layers = [nn.BatchNorm1d(n_in)] if bn else None\n","    if p != 0: layers.append(nn.Dropout(p))\n","    layers.append(nn.Linear(n_in, n_out))\n","    if activation is not None: layers.append(activation)\n","    return layers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKBx96xgUV44"},"source":["class ClassifierModel(nn.Module):\n","\n","    def __init__(self, embeddings, hidden_size, out_size, num_layers=1,\n","                 bidirectional=True, dropout_p=0.1, linears=None, drops=None,\n","                 activations=None, pad_idx=0, device=\"cpu\"):\n","        super(ClassifierModel, self).__init__()\n","\n","        self.device = device\n","        self.embeddings = embeddings\n","        self.hidden_size = hidden_size\n","        self.out_size = out_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.dropout_p = dropout_p\n","        self.pad_idx = pad_idx\n","\n","        self.dropout = nn.Dropout(p=dropout_p)\n","        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size,\n","                            num_layers=num_layers, bidirectional=bidirectional,\n","                            batch_first=True)\n","\n","        if bidirectional: hidden_size *= 2\n","        if linears is None: linears = [64]\n","\n","        class_layers = []\n","        linears = [hidden_size * 3] + linears + [out_size]  # hidden_size*3 bcs of concat pooling.\n","        if drops is None: drops = [0.3] * (len(linears) - 1)\n","        if activations is None: activations = [nn.ReLU(inplace=True)] * (len(linears) - 2) + [None]\n","\n","        if len(linears) - 1 != len(drops):\n","            raise ValueError(\"Number of layers and dropout values don't match.\")\n","\n","        for n_in, n_out, p, actn in zip(linears[:-1], linears[1:], drops, activations):\n","            class_layers += bn_drop_linear(n_in, n_out, p=p, activation=actn)\n","\n","        self.classifier = nn.Sequential(*class_layers)\n","\n","    def forward(self, sents):\n","        sent_lens = torch.tensor([len(sent) for sent in sents])\n","        sents_tensor = to_tensor(sents, device=self.device)  # (max_seq_len, bs)\n","        sent_masks = generate_sent_masks(sents_tensor, sent_lens, device=self.device)\n","        \n","        X = self.embeddings(sents_tensor)\n","        X = self.dropout(X)\n","        X = pack_padded_sequence(X, sent_lens, batch_first=True)\n","        \n","        hidden_outputs, (h_n, c_n) = self.lstm(X)  # (h_n, c_n)\n","        hidden_outputs, _ = pad_packed_sequence(hidden_outputs, batch_first=True)\n","\n","        max_pool, avg_pool = masked_pooling(hidden_outputs, sent_masks)\n","        h_n = torch.cat((h_n[-2], h_n[-1], max_pool, avg_pool), dim=1)\n","\n","        X = self.dropout(h_n)\n","        X = self.classifier(X)\n","        return X\n","\n","    def save(self, path):\n","\n","        params = {\n","            \"args\": dict(hidden_size=self.hidden_size,\n","                         out_size=self.out_size,\n","                         num_layers=self.num_layers,\n","                         bidirectional=self.bidirectional,\n","                         dropout_p=self.dropout_p,\n","                         pad_idx=self.pad_idx),\n","            \"embeddings\": self.embeddings,\n","            \"state_dict\": self.state_dict(),\n","        }\n","        torch.save(params, path)\n","\n","    @staticmethod\n","    def load(path):\n","\n","        params = torch.load(path, map_location=lambda storage, loc: storage)\n","        args = params[\"args\"]\n","        embeddings = params[\"embeddings\"]\n","        model = ClassifierModel(embeddings, **args)\n","        model.load_state_dict(params[\"state_dict\"])\n","        return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkdeV1QeUcAf"},"source":["\n","def generate_prediction_mask(predictions, threshold):\n","    mask = np.zeros_like(predictions)\n","    mask[predictions > threshold] = 1\n","\n","    other_sum = np.sum(mask, axis=1)\n","    other_mask = mask[:, 29]\n","    other_zero_mask = np.logical_and(other_mask == 1, other_sum > 1)\n","    mask[other_zero_mask, 29] = 0\n","    return mask.astype(np.uint8)\n","\n","\n","def calculate_f1(y_true, y_pred):\n","    macro_f1 = metrics.f1_score(y_true, y_pred, average='macro')\n","    return macro_f1\n","\n","\n","def f1_threshold(y_true, logits):\n","    thresholds = np.linspace(0, 1, 21)\n","    f1_scores = []\n","    for threshold in thresholds:\n","        mask = generate_prediction_mask(logits, threshold=threshold)\n","        macro_f1 = metrics.f1_score(y_true, mask, average='macro')\n","        f1_scores.append(round(macro_f1, 4))\n","\n","    max_f1_idx = np.argmax(f1_scores)\n","    max_f1_threshold = thresholds[max_f1_idx]\n","    max_f1_score = f1_scores[max_f1_idx]\n","\n","    print(\"Best threshold/score pair: {}/{}\".format(max_f1_threshold, max_f1_score))\n","    print(f1_scores)\n","    return max_f1_score\n","\n","\n","def train_step(model, loss_fn, optimizer, data, scheduler=None, bs=32):\n","    total_loss = 0.0\n","    model.train()\n","    start_time = time.time()\n","    total_batch = math.ceil(len(data) / bs)\n","\n","    for step, batch in enumerate(batch_iter(data, bs, shuffle=True)):\n","\n","        if step % 500 == 0 and not step == 0:\n","            elapsed_since = time.time() - start_time\n","            print(\"Batch {}/{}\\tElapsed since: {}\".format(step, total_batch,\n","                                                          str(datetime.timedelta(seconds=round(elapsed_since)))))\n","\n","        sents, labels = batch\n","        labels = torch.from_numpy(np.vstack(labels)).float()\n","        optimizer.zero_grad()\n","        logits = model(sents)\n","        train_loss = loss_fn(logits.cpu(), labels)\n","        total_loss += train_loss.item()\n","        train_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","    avg_train_loss = total_loss / total_batch\n","    return avg_train_loss\n","\n","\n","def eval_step(model, loss_fn, data, bs=32):\n","    total_loss = 0.0\n","    model.eval()\n","    total_batch = math.ceil(len(data) / bs)\n","    all_labels, all_logits = [], []\n","    with torch.no_grad():\n","      for batch in batch_iter(data, bs):\n","          sents, labels = batch\n","          labels = torch.from_numpy(np.vstack(labels)).float()\n","          logits = model(sents)\n","          eval_loss = loss_fn(logits.cpu(), labels)\n","          total_loss += eval_loss.item()\n","\n","          logits = torch.sigmoid(logits)\n","          all_labels.extend(labels.numpy().tolist())\n","          all_logits.extend(logits.cpu().detach().numpy().tolist())\n","\n","    average_eval_loss = total_loss / total_batch\n","    metrics = get_metrics(all_labels, all_logits)\n","    return average_eval_loss, metrics\n","\n","\n","def get_metrics(y_true, y_preds):\n","    precision = metrics.precision_score(y_true, np.round(y_preds), average='macro')\n","    recall = metrics.recall_score(y_true, np.round(y_preds), average='macro')\n","    \n","    return {\n","        \"f1\": metrics.f1_score(y_true, np.round(y_preds), average=\"macro\"),\n","        \"f2\": (5 * precision * recall) / (4 * precision + recall),\n","        \"roc_auc\": metrics.roc_auc_score(y_true, y_preds),\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"accuracy_score\": metrics.accuracy_score(y_true, np.round(y_preds)),\n","        \"classification_report\": metrics.classification_report(y_true, np.round(y_preds), labels=[0,1])\n","    }\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zt2eDkulT__K"},"source":["global excel_name\n","def train(model, loss_fn, optimizer, train_data, valid_data=None, scheduler=None, n_epochs=5, bs=32, model_kwargs=None):\n","    train_losses = []\n","    hist_valid_scores = []\n","    patience = 0\n","    total_patience = 5\n","    num_trial = 0\n","    max_num_trial = 5\n","    lr_decay = 0.5\n","    model_save_path = model_path = f\"drive/MyDrive/code-security/assets/models/{dt.now().strftime('%Y-%m-%d-%H-%M-%S')}_{n_epochs}_model.bin\"\n","    valid_niter = int((len(train_data) / bs)/2)\n","    best_model = None\n","    best_f1_model = None\n","    best_f1_all = 0.0\n","\n","    valid_losses = []\n","\n","    for epoch in range(n_epochs):\n","\n","        start_time = time.time()\n","\n","        total_loss = 0.0\n","        model.train()\n","        start_time = time.time()\n","        total_batch = math.ceil(len(train_data) / bs)\n","\n","        for step, batch in enumerate(batch_iter(train_data, bs, shuffle=True), start=1):\n","\n","            if step % 500 == 0 and not step == 0:\n","                elapsed_since = time.time() - start_time\n","                print(\"Batch {}/{}\\tElapsed since: {}\".format(step, total_batch,\n","                                                              str(datetime.timedelta(seconds=round(elapsed_since)))))\n","            sents, labels = batch\n","            labels = torch.from_numpy(np.vstack(labels)).float()\n","            optimizer.zero_grad()\n","            logits = model(sents)\n","            train_loss = loss_fn(logits.cpu(), labels)\n","            total_loss += train_loss.item()\n","            train_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n","            optimizer.step()\n","            if step % valid_niter == 0:  # \n","\n","                if valid_data is not None:\n","                    result_excel = load_workbook(\"drive/MyDrive/code-security/assets/results/binary/binary_classification_results_\"+str(excel_name)+\".xlsx\")\n","                    result_sheet = result_excel.active\n","                    excel_sheet_index = result_sheet.max_row + 1\n","                    result_sheet[\"A\" + str(excel_sheet_index)] = str(model_kwargs) + \" \\n \" + \"bs : \" + str(bs) + \" \\n\" + \"epoch : \" + str(n_epochs)\n","                    result_sheet[\"A\" + str(excel_sheet_index)].font = Font(bold=True)\n","\n","                    valid_loss, valid_metrics = eval_step(model, loss_fn, valid_data, bs=128)\n","                    valid_losses.append(valid_loss)\n","\n","                    result_sheet[\"A\" + str(excel_sheet_index + 1)] = \"Valid_loss\"\n","                    result_sheet[\"B\" + str(excel_sheet_index + 1)] = str(valid_loss)\n","                    excel_sheet_index += 1\n","\n","                    for metric, score in valid_metrics.items():\n","\n","                        result_sheet[\"A\" + str(excel_sheet_index)] = metric\n","                        result_sheet[\"B\" + str(excel_sheet_index)] = score\n","\n","                        excel_sheet_index += 1\n","\n","                    result_excel.save(\"drive/MyDrive/code-security/assets/results/binary/binary_classification_results_\"+str(excel_name)+\".xlsx\")\n","\n","                    is_better = len(hist_valid_scores) == 0 or valid_loss < min(hist_valid_scores)\n","                    hist_valid_scores.append(valid_loss)\n","                    if valid_metrics[\"f1\"] > best_f1_all:\n","                        best_f1_all = valid_metrics[\"f1\"]\n","                        best_f1_model = copy.deepcopy(model)\n","\n","                    if is_better:\n","                        patience = 0\n","                        model.save(model_save_path)\n","                        torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","\n","                    elif patience < total_patience:\n","                        patience += 1\n","                        print(\"Hit patience %d\" % patience, file=sys.stderr)\n","\n","                        if patience == total_patience:\n","                            num_trial += 1\n","                            print(\"Hit #%d trial\" % num_trial, file=sys.stderr)\n","\n","                            if num_trial == max_num_trial:\n","                                print(\"EARLY STOP!\", file=sys.stderr)\n","                                return train_losses, valid_losses if valid_data is not None else []\n","\n","                            lr = optimizer.param_groups[0]['lr'] * lr_decay\n","\n","                            for param_group in optimizer.param_groups:\n","                                param_group[\"lr\"] = lr\n","\n","                            patience = 0\n","\n","                    model.train()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","\n","      \n","        train_loss = total_loss / total_batch\n","        train_losses.append(train_loss)\n","\n","        elapsed_time = time.time() - start_time\n","        print(\"Epoch {}/{} is done. Took: {} Loss: {:.5f}\".format(epoch + 1,\n","                                                                  n_epochs,\n","                                                                  str(datetime.timedelta(seconds=round(elapsed_time))),\n","                                                                  train_loss))\n","\n","    return best_f1_model, train_losses, valid_losses if valid_data is not None else []\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbnyRPPZUgpR"},"source":["def main():\n","\n","    DEBUG = False\n","    tokenizer = Tokenizer.from_file(\"drive/MyDrive/code-security/datasets/word_embedding_model/notmarked/bpe_30k_no_mark_all_types.json\")\n","\n","    train_data = pd.read_csv(\"drive/MyDrive/code-security/datasets/binary_classification/train/notmarked/notmarked_sard_vdisc_train.csv\")\n","    valid_data = pd.read_csv(\"drive/MyDrive/code-security/datasets/binary_classification/validation/notmarked_sard_and_vdisc_validate.csv\")\n","\n","    if DEBUG:\n","        train_data = train_data.sample(1000, random_state=42)\n","        valid_data = valid_data.sample(1000, random_state=42)\n","\n","    \n","    global word2index\n","    global embedding\n","\n","    train_sents = tokenizer.encode_batch(train_data[\"code\"])\n","    train_sents = [item.tokens for item in train_sents]\n","\n","    train_labels = train_data[\"Label\"].values\n","\n","    valid_sents = tokenizer.encode_batch(valid_data[\"code\"])\n","    valid_sents = [item.tokens for item in valid_sents]\n","\n","    valid_labels = valid_data[\"Label\"].values\n","\n","    wv_fn = \"drive/MyDrive/code-security/datasets/word_embedding_model/notmarked/all_data_git_no_mark_sg_e_10_vec_300.model\"  # Vec file word2vec\n","    w2v_model = load_wv(wv_fn, fasttext=False)\n","\n","    additional_vectors = np.zeros(shape=(2, 300))\n","    index2word = [\"<pad>\", \"<unk>\"] + w2v_model.wv.index_to_key\n","    word2index = {word: index for index, word in enumerate(index2word)}\n","    weights = np.concatenate((additional_vectors, w2v_model.wv.vectors))\n","\n","    weights = torch.from_numpy(weights).float()\n","    embedding = nn.Embedding.from_pretrained(weights, padding_idx=0)\n","\n","    train_samples = list(zip(train_sents, train_labels))\n","    valid_samples = list(zip(valid_sents, valid_labels))\n","\n","    HIDDEN_SIZE = 128\n","    NUM_LAYERS = 2\n","    BIDIRECTIONAL = True\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    model_kwargs = {\n","        \"hidden_size\": HIDDEN_SIZE,\n","        \"out_size\": 1,\n","        \"dropout_p\": 0.3,\n","        \"num_layers\": NUM_LAYERS,\n","        \"bidirectional\": BIDIRECTIONAL,\n","        #     \"linears\": [64],\n","        #     \"drops\":,\n","        #     \"drops\": [0.3],\n","        \"device\": device\n","    }\n","\n","    model = ClassifierModel(embedding, **model_kwargs)\n","    model.to(device)\n","\n","    uniform_init = 0.1\n","    print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n","    for p in model.parameters():\n","        p.data.uniform_(-uniform_init, uniform_init)\n","\n","    optimizer = optim.Adam(model.parameters())\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    best_f1_model, train_losses, valid_losses = train(model, criterion, optimizer, train_samples,\n","                                                      valid_data=valid_samples, scheduler=None,\n","                                                      n_epochs=5, bs=32, model_kwargs=model_kwargs)\n","\n","    if best_f1_model != None:\n","      best_f1_model.save(f\"drive/MyDrive/code-security/assets/best_f1_models/{dt.now().strftime('%Y-%m-%d-%H-%M-%S')}_best_f1_model.bin\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LUPngPIKUlsa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e3a7a40-4793-4ed5-f07e-50a97cb7db4f"},"source":["import random\n","import os \n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything()\n","main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Label counts for the training data:\n","0    950214\n","1    112301\n","Name: Label, dtype: int64\n","Label counts for the validation data:\n","0    118868\n","1     14982\n","Name: Label, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["uniformly initialize parameters [-0.100000, +0.100000]\n"]},{"output_type":"stream","name":"stdout","text":["...Training for 5 epochs...\n","Number of training samples:  1062515\n","Batch 500/33204\tElapsed since: 1:01:25\n","Batch 1000/33204\tElapsed since: 1:55:22\n","Batch 1500/33204\tElapsed since: 2:49:33\n","Batch 2000/33204\tElapsed since: 3:41:23\n","Batch 2500/33204\tElapsed since: 4:36:34\n","Batch 3000/33204\tElapsed since: 5:28:55\n","Batch 3500/33204\tElapsed since: 6:22:12\n","Batch 4000/33204\tElapsed since: 7:14:36\n","Batch 4500/33204\tElapsed since: 8:18:56\n","Batch 5000/33204\tElapsed since: 9:28:34\n","Batch 5500/33204\tElapsed since: 10:40:12\n"]}]},{"cell_type":"code","metadata":{"id":"ZzKgFNd4gbQO"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jz5i1EVSIJDa"},"source":["# result_excel = load_workbook(\"drive/MyDrive/code-security/assets/results/binary/binary_classification_results.xlsx\")\n","# result_sheet = result_excel.active\n","# excel_sheet_index = result_sheet.max_row + 1\n","# print(excel_sheet_index)\n","# # result_sheet[\"A\" + str(excel_sheet_index)] =  \"dsfsdf\"\n","# # result_excel.save(\"drive/MyDrive/code-security/assets/results/binary/binary_classification_results.xlsx\")\n","\n","# last_empty_row = len(list(result_sheet.rows))\n","# last_empty_row\n","# len(result_sheet['A'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qx46Pa4FThDb"},"source":["valid_data[valid_data['CWE-469'] == True]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2Gzv4IixgaD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlD2jEejzXsr"},"source":[""],"execution_count":null,"outputs":[]}]}